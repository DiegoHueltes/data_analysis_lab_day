{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem\n",
    "\n",
    "We've been tasked by our head of data science to create a demo machine learning model that takes four measurements from the flowers (sepal length, sepal width, petal length, and petal width) and identifies the species based on those measurements alone.\n",
    "\n",
    "<img src=\"images/petal_sepal.jpg\" />\n",
    "\n",
    "We've been given a [data set](iris-data.csv) from our field researchers to develop the demo, which only includes measurements for three types of *Iris* flowers:\n",
    "\n",
    "### *Iris setosa*\n",
    "\n",
    "<img src=\"images/iris_setosa.jpg\" />\n",
    "\n",
    "### *Iris versicolor*\n",
    "<img src=\"images/iris_versicolor.jpg\" />\n",
    "\n",
    "### *Iris virginica*\n",
    "<img src=\"images/iris_virginica.jpg\" />\n",
    "\n",
    "The four measurements we're using currently come from hand-measurements by the field researchers, but they will be automatically measured by an image processing model in the future.\n",
    "\n",
    "### What is our objective?\n",
    "\n",
    "### What is the metric for success?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "iris_data = pd.read_csv('iris-data.csv', na_filter=None)\n",
    "iris_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're in luck! The data seems to be in a usable format.\n",
    "\n",
    "Each row following the first row represents an entry for a flower: four measurements and one class, which tells us the species of the flower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris_data = pd.read_csv('iris-data.csv', na_values=['NA'])\n",
    "iris_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voilà! Now pandas knows to treat rows with 'NA' as missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, it's always a good idea to look at the **distribution of our data** — especially the outliers.\n",
    "\n",
    "Let's start by printing out some summary statistics about the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see several useful values from this table. For example, we see that five `petal_width_cm` entries are missing.\n",
    "\n",
    "Visualization makes outliers and errors immediately stand out, whereas they might go unnoticed in a large table of numbers.\n",
    "\n",
    "Since we know we're going to be plotting in this section, let's set up the notebook so we can plot inside of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This line tells the notebook to show plots inside of the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create a **scatterplot matrix**. Scatterplot matrices plot the distribution of each column along the diagonal, and then plot a scatterplot matrix for the combination of each variable. They make for an efficient tool to look for errors in our data.\n",
    "\n",
    "We can even have the plotting package color each entry by its class to look for trends within the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sb.pairplot(iris_data.dropna(), hue='class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. There are five classes when there should only be three, meaning there were some coding errors.\n",
    "\n",
    "2. There are some clear outliers in the measurements that may be erroneous: one `sepal_width_cm` entry for `Iris-setosa` falls well outside its normal range, and several `sepal_length_cm` entries for `Iris-versicolor` are near-zero for some reason.\n",
    "\n",
    "3. We had to drop those rows with missing values.\n",
    "\n",
    "In all of these cases, we need to figure out what to do with the erroneous data. Which takes us to the next step..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris_data.loc[iris_data['class'] == 'versicolor', 'class'] = 'Iris-versicolor'\n",
    "iris_data.loc[iris_data['class'] == 'Iris-setossa', 'class'] = 'Iris-setosa'\n",
    "\n",
    "iris_data['class'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better! Now we only have three class types. Imagine how embarrassing it would've been to create a model that used the wrong classes.\n",
    "\n",
    ">There are some clear outliers in the measurements that may be erroneous: one `sepal_width_cm` entry for `Iris-setosa` falls well outside its normal range, and several `sepal_length_cm` entries for `Iris-versicolor` are near-zero for some reason.\n",
    "\n",
    "In the case of the one anomalous entry for `Iris-setosa`, let's say our field researchers know that it's impossible for `Iris-setosa` to have a sepal width below 2.5 cm. Clearly this entry was made in error, and we're better off just scrapping the entry than spending hours finding out what happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This line drops any 'Iris-setosa' rows with a separal width less than 2.5 cm\n",
    "iris_data = iris_data.loc[(iris_data['class'] != 'Iris-setosa') | (iris_data['sepal_width_cm'] >= 2.5)]\n",
    "iris_data.loc[iris_data['class'] == 'Iris-setosa', 'sepal_width_cm'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! Now all of our `Iris-setosa` rows have a sepal width greater than 2.5.\n",
    "\n",
    "The next data issue to address is the several near-zero sepal lengths for the `Iris-versicolor` rows. Let's take a look at those rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris_data.loc[(iris_data['class'] == 'Iris-versicolor') &\n",
    "              (iris_data['sepal_length_cm'] < 1.0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about that? All of these near-zero `sepal_length_cm` entries seem to be off by two orders of magnitude, as if they had been recorded in meters instead of centimeters.\n",
    "\n",
    "After some brief correspondence with the field researchers, we find that one of them forgot to convert those measurements to centimeters. Let's do that for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris_data.loc[(iris_data['class'] == 'Iris-versicolor') &\n",
    "              (iris_data['sepal_length_cm'] < 1.0),\n",
    "              'sepal_length_cm'] *= 100.0\n",
    "\n",
    "iris_data.loc[iris_data['class'] == 'Iris-versicolor', 'sepal_length_cm'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phew! Good thing we fixed those outliers. They could've really thrown our analysis off.\n",
    "\n",
    ">We had to drop those rows with missing values.\n",
    "\n",
    "Let's take a look at the rows with missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris_data.loc[(iris_data['sepal_length_cm'].isnull()) |\n",
    "              (iris_data['sepal_width_cm'].isnull()) |\n",
    "              (iris_data['petal_length_cm'].isnull()) |\n",
    "              (iris_data['petal_width_cm'].isnull())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not ideal that we had to drop those rows, especially considering they're all `Iris-setosa` entries. Since it seems like the missing data is systematic — all of the missing values are in the same column for the same *Iris* type — this error could potentially bias our analysis.\n",
    "\n",
    "One way to deal with missing data is **mean imputation**: If we know that the values for a measurement fall in a certain range, we can fill in empty values with the average of that measurement.\n",
    "\n",
    "Let's see if we can do that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris_data.loc[iris_data['class'] == 'Iris-setosa', 'petal_width_cm'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the petal widths for `Iris-setosa` fall within the 0.2-0.3 range, so let's fill in these entries with the average measured petal width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "average_petal_width = iris_data.loc[iris_data['class'] == 'Iris-setosa', 'petal_width_cm'].mean()\n",
    "\n",
    "iris_data.loc[(iris_data['class'] == 'Iris-setosa') &\n",
    "              (iris_data['petal_width_cm'].isnull()),\n",
    "              'petal_width_cm'] = average_petal_width\n",
    "\n",
    "iris_data.loc[(iris_data['class'] == 'Iris-setosa') &\n",
    "              (iris_data['petal_width_cm'] == average_petal_width)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris_data.loc[(iris_data['sepal_length_cm'].isnull()) |\n",
    "              (iris_data['sepal_width_cm'].isnull()) |\n",
    "              (iris_data['petal_length_cm'].isnull()) |\n",
    "              (iris_data['petal_width_cm'].isnull())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we've recovered those rows and no longer have missing data in our data set.\n",
    "\n",
    "**Note:** If you don't feel comfortable imputing your data, you can drop all rows with missing data with the `dropna()` call:\n",
    "\n",
    "    iris_data.dropna(inplace=True)\n",
    "\n",
    "After all this hard work, we don't want to repeat this process every time we work with the data set. Let's save the tidied data file *as a separate file* and work directly with that data file from now on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris_data.to_csv('iris-data-clean.csv', index=False)\n",
    "\n",
    "iris_data_clean = pd.read_csv('iris-data-clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a look at the scatterplot matrix now that we've tidied the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sb.pairplot(iris_data_clean, hue='class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Make sure your data is encoded properly\n",
    "\n",
    "* Make sure your data falls within the expected range, and use domain knowledge whenever possible to define that expected range\n",
    "\n",
    "* Deal with missing data in one way or another: replace it if you can or drop it\n",
    "\n",
    "* Never tidy your data manually because that is not easily reproducible\n",
    "\n",
    "* Use code as a record of how you tidied your data\n",
    "\n",
    "* Plot everything you can about the data at this stage of the analysis so you can *visually* confirm everything looks correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test a few things that we know about our data set now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We know that we should only have three classes\n",
    "assert len(iris_data_clean['class'].unique()) == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We know that sepal lengths for 'Iris-versicolor' should never be below 2.5 cm\n",
    "assert iris_data_clean.loc[iris_data_clean['class'] == 'Iris-versicolor', 'sepal_length_cm'].min() >= 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We know that our data set should have no missing measurements\n",
    "assert len(iris_data_clean.loc[(iris_data_clean['sepal_length_cm'].isnull()) |\n",
    "                               (iris_data_clean['sepal_width_cm'].isnull()) |\n",
    "                               (iris_data_clean['petal_length_cm'].isnull()) |\n",
    "                               (iris_data_clean['petal_width_cm'].isnull())]) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so on. If any of these expectations are violated, then our analysis immediately stops and we have to return to the tidying stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How is my data distributed?\n",
    "\n",
    "* Are there any correlations in my data?\n",
    "\n",
    "* Are there any confounding factors that explain these correlations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is normally distributed for the most part, which is great news if we plan on using any modeling methods that assume the data is normally distributed.\n",
    "\n",
    "There's something strange going with the petal measurements. Maybe it's something to do with the different `Iris` types. Let's color code the data by the class again to see if that clears things up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sb.pairplot(iris_data_clean, hue='class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure enough, the strange distribution of the petal measurements exist because of the different species. This is actually great news for our classification task since it means that the petal measurements will make it easy to distinguish between `Iris-setosa` and the other `Iris` types.\n",
    "\n",
    "Distinguishing `Iris-versicolor` and `Iris-virginica` will prove more difficult given how much their measurements overlap.\n",
    "\n",
    "There are also correlations between petal length and petal width, as well as sepal length and sepal width. The field biologists assure us that this is to be expected: Longer flower petals also tend to be wider, and the same applies for sepals.\n",
    "\n",
    "We can also make **violin plots** of the data to compare the measurement distributions of the classes. Violin plots contain the same information as [box plots](https://en.wikipedia.org/wiki/Box_plot), but also scales the box according to the density of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for column_index, column in enumerate(iris_data_clean.columns):\n",
    "    if column == 'class':\n",
    "        continue\n",
    "    plt.subplot(2, 2, column_index + 1)\n",
    "    sb.violinplot(x='class', y=column, data=iris_data_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enough flirting with the data. Let's get to modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "Remember: **Bad data leads to bad models.** Always check your data first.\n",
    "\n",
    "<hr />\n",
    "\n",
    "A **training set** is a random subset of the data that we use to train our models.\n",
    "\n",
    "A **testing set** is a random subset of the data (mutually exclusive from the training set) that we use to validate our models on unforseen data.\n",
    "\n",
    "Especially in sparse data sets like ours, it's easy for models to **overfit** the data: The model will learn the training set so well that it won't be able to handle most of the cases it's never seen before. This is why it's important for us to build the model with the training set, but score it with the testing set.\n",
    "\n",
    "Note that once we split the data into a training and testing set, we should treat the testing set like it no longer exists: We cannot use any information from the testing set to build our model or else we're cheating.\n",
    "\n",
    "Let's set up our data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris_data_clean = pd.read_csv('iris-data-clean.csv')\n",
    "\n",
    "# We can extract the data in this format from pandas like this:\n",
    "all_inputs = iris_data_clean[['sepal_length_cm', 'sepal_width_cm',\n",
    "                             'petal_length_cm', 'petal_width_cm']].values\n",
    "\n",
    "# Similarly, we can extract the classes\n",
    "all_classes = iris_data_clean['class'].values\n",
    "\n",
    "# Here's what a subset of our inputs looks like:\n",
    "all_inputs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our data is ready to be split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "(training_inputs,\n",
    " testing_inputs,\n",
    " training_classes,\n",
    " testing_classes) = train_test_split(all_inputs, all_classes, train_size=0.75, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our data split, we can start fitting models to our data. Our head of data is all about decision tree classifiers, so let's start with one of those.\n",
    "\n",
    "Decision tree classifiers are incredibly simple in theory. In their simplest form, decision tree classifiers ask a series of Yes/No questions about the data — each time getting closer to finding out the class of each entry — until they either classify the data set perfectly or simply can't differentiate a set of entries. Think of it like a game of [Twenty Questions](https://en.wikipedia.org/wiki/Twenty_Questions), except the computer is *much*, *much* better at it.\n",
    "\n",
    "Here's an example decision tree classifier:\n",
    "\n",
    "<img src=\"iris_dtc.png\" />\n",
    "\n",
    "Notice how the classifier asks Yes/No questions about the data — whether a certain feature is <= 1.75, for example — so it can differentiate the records. This is the essence of every decision tree.\n",
    "\n",
    "The nice part about decision tree classifiers is that they are **scale-invariant**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Create the classifier\n",
    "decision_tree_classifier = DecisionTreeClassifier()\n",
    "\n",
    "# Train the classifier on the training set\n",
    "decision_tree_classifier.fit(training_inputs, training_classes)\n",
    "\n",
    "# Validate the classifier on the testing set using classification accuracy\n",
    "decision_tree_classifier.score(testing_inputs, testing_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heck yeah! Our model achieves 97% classification accuracy without much effort.\n",
    "\n",
    "However, there's a catch: Depending on how our training and testing set was sampled, our model can achieve anywhere from 80% to 100% accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_accuracies = []\n",
    "\n",
    "for repetition in range(1000):\n",
    "    (training_inputs,\n",
    "     testing_inputs,\n",
    "     training_classes,\n",
    "     testing_classes) = train_test_split(all_inputs, all_classes, train_size=0.75)\n",
    "    \n",
    "    decision_tree_classifier = DecisionTreeClassifier()\n",
    "    decision_tree_classifier.fit(training_inputs, training_classes)\n",
    "    classifier_accuracy = decision_tree_classifier.score(testing_inputs, testing_classes)\n",
    "    model_accuracies.append(classifier_accuracy)\n",
    "    \n",
    "plt.xlim([0.7, 1.2])\n",
    "sb.distplot(model_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's obviously a problem that our model performs quite differently depending on the subset of the data it's trained on. This phenomenon is known as **overfitting**: The model is learning to classify the training set so well that it doesn't generalize and perform well on data it hasn't seen before.\n",
    "\n",
    "### Cross-validation\n",
    "\n",
    "10-fold cross-validation is the most common choice, so let's use that here. Performing 10-fold cross-validation on our data set looks something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "\n",
    "def plot_cv(cv, n_samples):\n",
    "    masks = []\n",
    "    for train, test in cv:\n",
    "        mask = np.zeros(n_samples, dtype=bool)\n",
    "        mask[test] = 1\n",
    "        masks.append(mask)\n",
    "        \n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.imshow(masks, interpolation='none')\n",
    "    plt.ylabel('Fold')\n",
    "    plt.xlabel('Row #')\n",
    "\n",
    "plot_cv(StratifiedKFold(all_classes, n_folds=10), len(all_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "decision_tree_classifier = DecisionTreeClassifier()\n",
    "\n",
    "# cross_val_score returns a list of the scores, which we can visualize\n",
    "# to get a reasonable estimate of our classifier's performance\n",
    "cv_scores = cross_val_score(decision_tree_classifier, all_inputs, all_classes, cv=10)\n",
    "sb.distplot(cv_scores)\n",
    "plt.xlim([0.5, 1.5])\n",
    "plt.title('Average score: {}'.format(np.mean(cv_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a much more consistent rating of our classifier's general classification accuracy.\n",
    "\n",
    "### Parameter tuning\n",
    "\n",
    "Every Machine Learning model comes with a variety of parameters to tune, and these parameters can be vitally important to the performance of our classifier. For example, if we severely limit the depth of our decision tree classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decision_tree_classifier = DecisionTreeClassifier(max_depth=1)\n",
    "\n",
    "cv_scores = cross_val_score(decision_tree_classifier, all_inputs, all_classes, cv=10)\n",
    "sb.distplot(cv_scores, kde=False)\n",
    "plt.title('Average score: {}'.format(np.mean(cv_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the classification accuracy falls tremendously.\n",
    "\n",
    "Therefore, we need to find a systematic method to discover the best parameters for our model and data set.\n",
    "\n",
    "The most common method for model parameter tuning is **Grid Search**. The idea behind Grid Search is simple: explore a range of parameters and find the best-performing parameter combination. Focus your search on the best range of parameters, then repeat this process several times until the best parameters are discovered.\n",
    "\n",
    "Let's tune our decision tree classifier. We'll stick to only two parameters for now, but it's possible to simultaneously explore dozens of parameters if we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "decision_tree_classifier = DecisionTreeClassifier()\n",
    "\n",
    "parameter_grid = {'max_depth': [1, 2, 3, 4, 5],\n",
    "                  'max_features': [1, 2, 3, 4]}\n",
    "\n",
    "cross_validation = StratifiedKFold(all_classes, n_folds=10)\n",
    "\n",
    "grid_search = GridSearchCV(decision_tree_classifier,\n",
    "                           param_grid=parameter_grid,\n",
    "                           cv=cross_validation)\n",
    "\n",
    "grid_search.fit(all_inputs, all_classes)\n",
    "print('Best score: {}'.format(grid_search.best_score_))\n",
    "print('Best parameters: {}'.format(grid_search.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize the grid search to see how the parameters interact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid_visualization = []\n",
    "\n",
    "for grid_pair in grid_search.grid_scores_:\n",
    "    grid_visualization.append(grid_pair.mean_validation_score)\n",
    "    \n",
    "grid_visualization = np.array(grid_visualization)\n",
    "grid_visualization.shape = (5, 4)\n",
    "sb.heatmap(grid_visualization, cmap='Blues')\n",
    "plt.xticks(np.arange(4) + 0.5, grid_search.param_grid['max_features'])\n",
    "plt.yticks(np.arange(5) + 0.5, grid_search.param_grid['max_depth'][::-1])\n",
    "plt.xlabel('max_features')\n",
    "plt.ylabel('max_depth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a better sense of the parameter space: We know that we need a `max_depth` of at least 2 to allow the decision tree to make more than a one-off decision.\n",
    "\n",
    "`max_features` doesn't really seem to make a big difference here as long as we have 2 of them, which makes sense since our data set has only 4 features and is relatively easy to classify. (Remember, one of our data set's classes was easily separable from the rest based on a single feature.)\n",
    "\n",
    "Let's go ahead and use a broad grid search to find the best settings for a handful of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decision_tree_classifier = DecisionTreeClassifier()\n",
    "\n",
    "parameter_grid = {'criterion': ['gini', 'entropy'],\n",
    "                  'splitter': ['best', 'random'],\n",
    "                  'max_depth': [1, 2, 3, 4, 5],\n",
    "                  'max_features': [1, 2, 3, 4]}\n",
    "\n",
    "cross_validation = StratifiedKFold(all_classes, n_folds=10)\n",
    "\n",
    "grid_search = GridSearchCV(decision_tree_classifier,\n",
    "                           param_grid=parameter_grid,\n",
    "                           cv=cross_validation)\n",
    "\n",
    "grid_search.fit(all_inputs, all_classes)\n",
    "print('Best score: {}'.format(grid_search.best_score_))\n",
    "print('Best parameters: {}'.format(grid_search.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take the best classifier from the Grid Search and use that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decision_tree_classifier = grid_search.best_estimator_\n",
    "decision_tree_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even visualize the decision tree with [GraphViz](http://www.graphviz.org/) to see how it's making the classifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn.tree as tree\n",
    "from sklearn.externals.six import StringIO\n",
    "\n",
    "with open('iris_dtc.dot', 'w') as out_file:\n",
    "    out_file = tree.export_graphviz(decision_tree_classifier, out_file=out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<img src=\"iris_dtc.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(This classifier may look familiar from earlier in the notebook.)\n",
    "\n",
    "Alright! We finally have our demo classifier. Let's create some visuals of its performance so we have something to show our head of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest classifiers** work around that limitation by creating a whole bunch of decision trees (hence \"forest\") — each trained on random subsets of training samples (drawn with replacement) and features (drawn without replacement) — and have the decision trees work together to make a more accurate classification.\n",
    "\n",
    "**We get better results when we work together!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_forest_classifier = RandomForestClassifier()\n",
    "\n",
    "parameter_grid = {'n_estimators': [5, 10, 25, 50],\n",
    "                  'criterion': ['gini', 'entropy'],\n",
    "                  'max_features': [1, 2, 3, 4],\n",
    "                  'warm_start': [True, False]}\n",
    "\n",
    "cross_validation = StratifiedKFold(all_classes, n_folds=10)\n",
    "\n",
    "grid_search = GridSearchCV(random_forest_classifier,\n",
    "                           param_grid=parameter_grid,\n",
    "                           cv=cross_validation)\n",
    "\n",
    "grid_search.fit(all_inputs, all_classes)\n",
    "print('Best score: {}'.format(grid_search.best_score_))\n",
    "print('Best parameters: {}'.format(grid_search.best_params_))\n",
    "\n",
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://imgs.xkcd.com/comics/python.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [tpmnb.org](https://tmpnb.org)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
